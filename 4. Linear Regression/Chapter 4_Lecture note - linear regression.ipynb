{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "※ Coursera 강의에서 구체적으로 정리하므로 간단하게 필기하는 것으로 넘어갈 것!\n",
    "### Gradient decent based learning   \n",
    "- 실제 값과 학습된 모델 예측치의 오차 최소화\n",
    "\n",
    "### Cost function\n",
    "- 예측 함수 = 가설 함수\n",
    "- Cost function: 실제값과 가설함수의 차이\n",
    "- Goal: cost function을 최소화하기 위한 weight 값\n",
    "- How to find weight?: 연립방정식(Normal equation)\n",
    "\n",
    "### Normal equation\n",
    "- XTX의 역행렬이 존재할 때 사용\n",
    "- Iteration 등 사용자 지정 parameter가 ㅓㅇㅄ음\n",
    "- Feature가 많으면 계산이 느려짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, fit_intercept=True, copy_X=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.copy_X = copy_X\n",
    "\n",
    "        self._coef = None\n",
    "        self._intercept = None\n",
    "        self._new_X = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Linear regression 모델을 적합한다.\n",
    "        Matrix X와 Vector Y가 입력 값으로 들어오면 Normal equation을 활용하여, weight값을\n",
    "        찾는다. 이 때, instance가 생성될 때, fit_intercept 설정에 따라 fit 실행이 달라진다.\n",
    "        fit을 할 때는 입력되는 X의 값은 반드시 새로운 변수(self._new_X)에 저장\n",
    "        된 후 실행되어야 한다.\n",
    "        fit_intercept가 True일 경우:\n",
    "            - Matrix X의 0번째 Column에 값이 1인 column vector를추가한다.\n",
    "        적합이 종료된 후 각 변수의 계수(coefficient 또는 weight값을 의미)는 self._coef와\n",
    "        self._intercept_coef에 저장된다. 이때 self._coef는 numpy array을 각 변수항의\n",
    "        weight값을 저장한 1차원 vector이며, self._intercept_coef는 상수항의 weight를\n",
    "        저장한 scalar(float) 이다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, 2차원 matrix 형태로 [n_samples,n_features] 구조를 가진다\n",
    "        y : numpy array, 1차원 vector 형태로 [n_targets]의 구조를 가진다.\n",
    "        Returns\n",
    "        -------\n",
    "        self : 현재의 인스턴스가 리턴된다\n",
    "        \"\"\"\n",
    "        self._new_X = np.array(X)\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            intercept_vector = np.ones([len(self._new_X), 1])\n",
    "            self._new_X = np.concatenate((intercept_vector, self._new_X), axis=1)\n",
    "        \n",
    "        weights = np.linalg.inv(self._new_X.T.dot(self._newX_)).dot(self._new_X.T.dot(y)).flatten()\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            self._intercept = weights[0]\n",
    "            self._coef = weights[1:]\n",
    "        else:\n",
    "            self._coef = weights\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        적합된 Linear regression 모델을 사용하여 입력된 Matrix X의 예측값을 반환한다.\n",
    "        이 때, 입력된 Matrix X는 별도의 전처리가 없는 상태로 입력되는 걸로 가정한다.\n",
    "        fit_intercept가 True일 경우:\n",
    "            - Matrix X의 0번째 Column에 값이 1인 column vector를추가한다.\n",
    "        normalize가 True일 경우:\n",
    "            - Standard normalization으로 Matrix X의 column 0(상수)를 제외한 모든 값을\n",
    "              정규화을 실행함\n",
    "            - 정규화를 할때는 self._mu_X와 self._std_X 에 있는 값을 사용한다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, 2차원 matrix 형태로 [n_samples,n_features] 구조를 가진다\n",
    "        Returns\n",
    "        -------\n",
    "        y : numpy array, 예측된 값을 1차원 vector 형태로 [n_predicted_targets]의\n",
    "            구조를 가진다.\n",
    "        \"\"\"\n",
    "        test_X = np.array(X)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            intercept_vector = np.ones([len(test_X),1])\n",
    "            test_X = np.concatenate((intercept_vector, test_X), axis=1)\n",
    "            \n",
    "            weights = np.concatenate(([self._intercept], self._coef), axis=0)\n",
    "        else:\n",
    "            weights = self._coef\n",
    "        \n",
    "        return test_X.dot(weights)\n",
    "\n",
    "    @property\n",
    "    def coef(self):\n",
    "        return self._coef\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_desecnt(X,y,w,alpha,iterations): #alpha = learning rate\n",
    "    theta = w\n",
    "    m = len(y)\n",
    "    theta_list = [theta.tolist()]\n",
    "    cost = cost_function(hypothesis_function(X,theta),y)\n",
    "    cost_list = [cost]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        t0 = theta[0] - (alpha / m) * np.sum(np.dot(X,theta)-y)\n",
    "        t1 = theta[1] - (alpha / m) * np.sum((np.dot(X,theta)-y) * X[:,1])\n",
    "        theta = np.array([t0,t1])\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            theta_list.append(theta.tolist())\n",
    "            cost = cost_function(hypothesis_function(X, theta), y)\n",
    "            cost_list.append(cost)\n",
    "            \n",
    "    return theta, theta_list, cost_list      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
